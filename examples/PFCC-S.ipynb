{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "pairs = pd.read_csv('datasets/Snopes/tweet-veclaim-pairs.tsv', sep='\\t')\n",
    "pairs[\"score\"] = 1 #[0.8 for _ in range(400)] + [1 for _ in range(len(pairs) - 400)]\n",
    "pairs[:800].to_csv('datasets/Snopes/train.tsv', sep='\\t', header=None, index=False)\n",
    "test = pairs[800:]\n",
    "veclaims = pd.read_csv('datasets/Snopes/verified-claims.tsv', sep='\\t')\n",
    "new_test = test.copy()\n",
    "for i in range(5):\n",
    "    add_test = pd.DataFrame({'claim': test['claim'].values, 'fact': veclaims['fact'].sample(len(test)), 'score': 0})\n",
    "    new_test = pd.concat([new_test, add_test])\n",
    "new_test.to_csv('datasets/Snopes/test.tsv', sep='\\t', header=None, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"../lib\")\n",
    "\n",
    "from STSDataReaderBinary import STSDataReaderBinary\n",
    "from STSDataReaderBinaryPositives import STSDataReaderBinaryPositives\n",
    "from BSCLoss import BSCLoss, ComboBSCLoss\n",
    "from BSCShuffler import ShuffledSentencesDataset, ShuffledSentenceTransformer\n",
    "from BSCShuffler import BSCShuffler, ModelBSCShuffler, ModelExampleBasedShuffler\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "import math\n",
    "import os\n",
    "from sentence_transformers import models, losses\n",
    "from sentence_transformers import SentencesDataset, LoggingHandler, SentenceTransformer, evaluation\n",
    "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator, TripletEvaluator, SimilarityFunction\n",
    "from sentence_transformers.readers import *\n",
    "import pandas as pd\n",
    "import logging\n",
    "import csv\n",
    "from unidecode import unidecode\n",
    "\n",
    "#### Just some code to print debug information to stdout\n",
    "logging.basicConfig(format='%(asctime)s - %(message)s',\n",
    "                    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "                    level=logging.INFO,\n",
    "                    handlers=[LoggingHandler()])\n",
    "\n",
    "num_runs = 5\n",
    "\n",
    "\n",
    "def clear_text(text):\n",
    "    text = unidecode(text)\n",
    "    text = text.replace(\"'\", '\"')\n",
    "    return ' '.join(text.strip())\n",
    "\n",
    "\n",
    "def has_positive(preds, k, el):\n",
    "    for pred in preds[:k]:\n",
    "        if clear_text(el) == clear_text(pred):\n",
    "            return 1\n",
    "    return 0\n",
    "\n",
    "def has_positive_old(preds, k, el):\n",
    "    return int(el in preds[:k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('intervals_estimates/new_metrics-Snopes.txt', 'a+') as f:\n",
    "    f.write('bsc, ex-based, 7-same, 2e-5, bias False, norm 0, tau 1.2\\n')\n",
    "for _ in range(num_runs):\n",
    "    word_embedding_model = models.Transformer('bert-base-uncased', max_seq_length=90)\n",
    "    pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(),\n",
    "                                   pooling_mode_mean_tokens=True,\n",
    "                                   pooling_mode_cls_token=False,\n",
    "                                   pooling_mode_max_tokens=False)\n",
    "    model = ShuffledSentenceTransformer(modules=[word_embedding_model, pooling_model], device='cuda')\n",
    "\n",
    "    sts_reader_pos = STSDataReaderBinaryPositives('datasets/Snopes', \n",
    "                               s1_col_idx=0, s2_col_idx=1, score_col_idx=2,normalize_scores=False, thr=0.6,\n",
    "                                                 get_positives=False)\n",
    "    sts_reader = STSDataReader('datasets/Snopes', \n",
    "                               s1_col_idx=0, s2_col_idx=1, score_col_idx=2,normalize_scores=False)\n",
    "\n",
    "    train_batch_size = 30\n",
    "    num_epochs = 6\n",
    "\n",
    "    train_examples = []\n",
    "    with open(os.path.join('datasets/Snopes/', 'train_triplets.tsv'), encoding=\"utf-8\") as fIn:\n",
    "        reader = csv.reader(fIn, delimiter='\\t', quoting=csv.QUOTE_MINIMAL)\n",
    "        for row in reader:\n",
    "            train_examples.append(InputExample(texts=[row[0], row[1], row[2]], label=1))\n",
    "\n",
    "    train_data_triplet = SentencesDataset(examples=train_examples, model=model)\n",
    "    train_dataloader_triplet = DataLoader(train_data_triplet, shuffle=True, batch_size=train_batch_size)\n",
    "    train_loss_triplet = losses.TripletLoss(model=model, triplet_margin=0.5)\n",
    "\n",
    "    train_data_bsc = ShuffledSentencesDataset(sts_reader_pos.get_examples('train.tsv'), model)\n",
    "    train_dataloader_bsc = DataLoader(train_data_bsc, shuffle=False, batch_size=train_batch_size)\n",
    "    train_loss_bsc = BSCLoss(model=model, norm_dim=0, tau=1.2)\n",
    "\n",
    "    train_data = SentencesDataset(sts_reader.get_examples('train.tsv'), model)\n",
    "    train_dataloader = DataLoader(train_data, shuffle=True, batch_size=train_batch_size)\n",
    "    train_loss = losses.CosineSimilarityLoss(model=model)\n",
    "\n",
    "    dev_sentences1 = []\n",
    "    dev_sentences2 = []\n",
    "    dev_labels = []\n",
    "    with open(os.path.join('datasets/Snopes/', \"test.tsv\"), encoding='utf8') as fIn:\n",
    "        for row in fIn.readlines():\n",
    "            row = row.split('\\t')\n",
    "            dev_sentences1.append(row[0])\n",
    "            dev_sentences2.append(row[1])\n",
    "            dev_labels.append(int(row[2]))\n",
    "    evaluator = evaluation.BinaryClassificationEvaluator(dev_sentences1, dev_sentences2, dev_labels)\n",
    "\n",
    "    shuffler = ModelExampleBasedShuffler(group_size=7, allow_same=True)\n",
    "    warmup_steps = math.ceil(len(train_data_bsc)*num_epochs/train_batch_size*0.1)\n",
    "    model_save_path = 'checkpoints_/bsc_snopes_shuffled'\n",
    "    get_ipython().system(\"rm -rf 'checkpoints_/bsc_snopes_shuffled'\")\n",
    "\n",
    "    model.fit(train_objectives=[(train_dataloader_bsc, train_loss_bsc)],\n",
    "              evaluator=evaluator,\n",
    "              epochs=num_epochs,\n",
    "              evaluation_steps=1000,\n",
    "              warmup_steps=warmup_steps,\n",
    "              output_path=model_save_path,\n",
    "              optimizer_params={'alpha_lr':0.4, 'lr': 2e-5, 'correct_bias': False},\n",
    "              shuffler=shuffler,\n",
    "              shuffle_idxs=[0]\n",
    "             )\n",
    "\n",
    "    embedder = SentenceTransformer('checkpoints_/bsc_snopes_shuffled') # device='cpu'\n",
    "\n",
    "    test = pd.read_csv('datasets/Snopes/test.tsv', sep='\\t', header=None)\n",
    "    test_left = embedder.encode(test[0].values)\n",
    "    gr_test = test_left[:199]\n",
    "    veclaims = pd.read_csv('datasets/Snopes/verified-claims.tsv', sep='\\t')\n",
    "    one_vs_all_right = embedder.encode(veclaims['fact'].values)\n",
    "\n",
    "    import numpy as np\n",
    "    from tqdm import tqdm\n",
    "    from sklearn.metrics.pairwise import paired_cosine_distances\n",
    "\n",
    "    sc = np.array([0, 0, 0, 0, 0, 0])\n",
    "    sc_old = np.array([0, 0, 0, 0, 0, 0])\n",
    "    for i in tqdm(range(len(gr_test))):\n",
    "        one_vs_all_left = [gr_test[i] for _ in range(len(veclaims))]\n",
    "        one_vs_all_sim = 1 - (paired_cosine_distances(one_vs_all_left, one_vs_all_right))\n",
    "        preds = [el[1] for el in sorted(zip(one_vs_all_sim, veclaims['fact'].values), key=lambda x: -x[0])]\n",
    "        sc += np.array([has_positive(preds, k, test.loc[i, 1]) for k in [1, 3, 5, 10, 20, 50]])\n",
    "        sc_old += np.array([has_positive_old(preds, k, test.loc[i, 1]) for k in [1, 3, 5, 10, 20, 50]])\n",
    "    \n",
    "    with open('intervals_estimates/new_metrics-Snopes.txt', 'a+') as f:\n",
    "        ks = [1, 3, 5, 10, 20, 50]\n",
    "        for i in range(len(ks)):\n",
    "            f.write('HasPositive@{} {}\\n'.format(ks[i], round((sc / len(gr_test))[i], 3)))\n",
    "        f.write('OLD\\n')\n",
    "        for i in range(len(ks)):\n",
    "            f.write('HasPositive@{} {}\\n'.format(ks[i], round((sc_old / len(gr_test))[i], 3)))\n",
    "        f.write('\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
