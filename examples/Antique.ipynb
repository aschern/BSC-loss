{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://ciir.cs.umass.edu/downloads/Antique/antique-collection.txt -P datasets/Antique "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "ant_table = []\n",
    "with open('datasets/Antique/antique-train.qrel', 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        line = line.replace(' ', '\\t').strip().split('\\t')\n",
    "        ant_table.append([line[0], line[1], line[2], line[3]])\n",
    "ant = pd.DataFrame(ant_table)\n",
    "\n",
    "ant_table = []\n",
    "with open('datasets/Antique/antique-train-queries.txt', 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        line = line.strip().split('\\t')\n",
    "        ant_table.append([line[0], ' '.join(line[1:])])\n",
    "ant_texts = pd.DataFrame(ant_table)\n",
    "ant_texts = dict(zip(ant_texts[0].values, ant_texts[1].values))\n",
    "\n",
    "ant_table = []\n",
    "with open('datasets/Antique/antique-collection.txt', 'r') as f:\n",
    "    for line in f.readlines():\n",
    "        line = line.strip().split('\\t')\n",
    "        ant_table.append([line[0], ' '.join(line[1:])])\n",
    "ant_texts = pd.DataFrame(ant_table)\n",
    "ant_texts = dict(zip(ant_texts[0].values, ant_texts[1].values))\n",
    "\n",
    "ant[5] = ant[2].apply(lambda x: ant_texts[x])\n",
    "ant.to_csv('datasets/Antique/train.tsv', sep='\\t', header=None, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"../lib\")\n",
    "\n",
    "from STSDataReaderBinary import STSDataReaderBinary\n",
    "from STSDataReaderBinaryPositives import STSDataReaderBinaryPositives\n",
    "from BSCLoss import BSCLoss, ComboBSCLoss\n",
    "from BSCShuffler import ShuffledSentencesDataset, ShuffledSentenceTransformer\n",
    "from BSCShuffler import BSCShuffler, ModelBSCShuffler, ModelExampleBasedShuffler\n",
    "from torch.utils.data import DataLoader\n",
    "import math\n",
    "import os\n",
    "\n",
    "from sentence_transformers import models, losses\n",
    "from sentence_transformers import SentencesDataset, LoggingHandler, SentenceTransformer, evaluation\n",
    "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator, TripletEvaluator, SimilarityFunction\n",
    "from sentence_transformers.readers import *\n",
    "import pandas as pd\n",
    "import logging\n",
    "import csv\n",
    "\n",
    "#### Just some code to print debug information to stdout\n",
    "logging.basicConfig(format='%(asctime)s - %(message)s',\n",
    "                    datefmt='%Y-%m-%d %H:%M:%S',\n",
    "                    level=logging.INFO,\n",
    "                    handlers=[LoggingHandler()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "word_embedding_model = models.Transformer('bert-base-uncased', max_seq_length=90)\n",
    "pooling_model = models.Pooling(word_embedding_model.get_word_embedding_dimension(),\n",
    "                               pooling_mode_mean_tokens=True,\n",
    "                               pooling_mode_cls_token=False,\n",
    "                               pooling_mode_max_tokens=False)\n",
    "\n",
    "model = ShuffledSentenceTransformer(modules=[word_embedding_model, pooling_model])\n",
    "\n",
    "\n",
    "sts_reader_pos = STSDataReaderBinaryPositives('datasets/Antique', \n",
    "                                              s1_col_idx=4, s2_col_idx=5, score_col_idx=3,\n",
    "                                              min_score=1, max_score=4, thr=0.5, normalize_scores=True,\n",
    "                                             get_positives=False)\n",
    "sts_reader = STSDataReader('datasets/Antique', \n",
    "                          s1_col_idx=4, s2_col_idx=5, score_col_idx=3,\n",
    "                          min_score=1, max_score=4, normalize_scores=True)\n",
    "\n",
    "\n",
    "train_batch_size = 50\n",
    "num_epochs = 5\n",
    "\n",
    "\n",
    "train_data_bsc = ShuffledSentencesDataset(sts_reader_pos.get_examples('train.tsv'), model)\n",
    "train_loss_bsc = BSCLoss(model=model, tau=0.1, norm_dim=1)\n",
    "train_dataloader_bsc = DataLoader(train_data_bsc, shuffle=False, batch_size=train_batch_size)\n",
    "\n",
    "train_data = SentencesDataset(sts_reader.get_examples('train.tsv'), model)\n",
    "train_loss = losses.CosineSimilarityLoss(model=model)\n",
    "train_dataloader = DataLoader(train_data, shuffle=True, batch_size=train_batch_size)\n",
    "\n",
    "evaluator = EmbeddingSimilarityEvaluator.from_input_examples(sts_reader.get_examples('test.tsv'), name='test')\n",
    "evaluator.device = 'cuda'\n",
    "evaluator.main_similarity = SimilarityFunction.COSINE\n",
    "\n",
    "warmup_steps = math.ceil(len(train_data)*num_epochs/train_batch_size*0.1)\n",
    "model_save_path = 'checkpoints/bsc_antique'\n",
    "\n",
    "shuffler = ModelExampleBasedShuffler(group_size=7, allow_same=True)\n",
    "\n",
    "model.fit(train_objectives=[(train_dataloader_bsc, train_loss_bsc)],\n",
    "          evaluator=evaluator,\n",
    "          epochs=num_epochs,\n",
    "          evaluation_steps=1000,\n",
    "          warmup_steps=warmup_steps,\n",
    "          # optimizer_params={'lr': 3e-5, 'eps': 1e-6, 'correct_bias': False},\n",
    "          shuffler=shuffler,\n",
    "          shuffle_idxs=[0],\n",
    "          output_path=model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers.util import batch_to_device\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics.pairwise import paired_cosine_distances\n",
    "import pandas as pd\n",
    "from metrics import calculate_metrics\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Predictor:\n",
    "    def __init__(self, dataloader, show_progress_bar):\n",
    "        self.dataloader = dataloader\n",
    "        if show_progress_bar is None:\n",
    "            show_progress_bar = (logging.getLogger().getEffectiveLevel() == logging.INFO or logging.getLogger().getEffectiveLevel() == logging.DEBUG)\n",
    "        self.show_progress_bar = show_progress_bar\n",
    "\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    def __call__(self, model):\n",
    "        model.eval()\n",
    "        embeddings1 = []\n",
    "        embeddings2 = []\n",
    "        labels = []\n",
    "\n",
    "        self.dataloader.collate_fn = model.smart_batching_collate\n",
    "\n",
    "        iterator = self.dataloader\n",
    "        if self.show_progress_bar:\n",
    "            iterator = tqdm(iterator, desc=\"Convert Evaluating\")\n",
    "\n",
    "        for step, batch in enumerate(iterator):\n",
    "            features, label_ids = batch_to_device(batch, self.device)\n",
    "            with torch.no_grad():\n",
    "                emb1, emb2 = [model(sent_features)['sentence_embedding'].to(\"cpu\").numpy() for sent_features in features]\n",
    "\n",
    "            labels.extend(label_ids.to(\"cpu\").numpy())\n",
    "            embeddings1.extend(emb1)\n",
    "            embeddings2.extend(emb2)\n",
    "\n",
    "        try:\n",
    "            cosine_scores = 1 - (paired_cosine_distances(embeddings1, embeddings2))\n",
    "        except Exception as e:\n",
    "            print(embeddings1)\n",
    "            print(embeddings2)\n",
    "            raise(e)\n",
    "        return cosine_scores, labels\n",
    "    \n",
    "model = SentenceTransformer('checkpoints/fps_antique')\n",
    "#model.evaluate(evaluator)\n",
    "\n",
    "dev_data = SentencesDataset(sts_reader.get_examples('test.tsv'), model)\n",
    "dev_dataloader = DataLoader(dev_data, shuffle=False, batch_size=train_batch_size)\n",
    "\n",
    "predictor = Predictor(dev_dataloader, show_progress_bar=True)\n",
    "preds, labels = predictor(model)\n",
    "pairs = list(zip(list(pd.read_csv('datasets/Antique/test.tsv', sep='\\t', header=None)[0].values),\n",
    "         list(pd.read_csv('datasets/Antique/test.tsv', sep='\\t', header=None)[2].values)))\n",
    "\n",
    "calculate_metrics(pairs, np.array(preds) * 3 + 1, np.array(labels) * 3 + 1, (np.array(labels) > 0.5).astype(int))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
